# configに全モデルのconfを書く

train:
  in_size: &in_size 64
  in_channel: &in_channel 2
  time_sequence: 100 #時系列は100step分
  batch: 64
  epoch: 30
  iter: -1 #ここは-1にするとmaxまで学習する
  lr: 0.001
  save_interval: 1 #epochごと
  datapath: /mnt/ssd1/hiranotakaya/master/dev/workspace/prj_202407_2MR/time_robust_snn_prj/framedata/DVSGesture #1倍速のデータのみ
  event_timewindow: 50000 #event_timewindowごとにeventをframe変換する (データ作るときのtoFrameの引数)


model:

  is-beta: true
  is-gamma: true #gammaを推定する

  ersnn: 
    in_size: *in_size
    in_channel: *in_channel
    out_channel: &out_channel 9
    kernel: 3
    stride: 1
    padding: 1
    pool_type: avg #poolingでサイズを半分にしてる
    is_bn: True
    mem_reset_type: subtract
    mem_threshold: 1.0
    gamma_in: &gamma_in 32 #betaのhとwサイズ
    gamma_size: [*out_channel,*gamma_in,*gamma_in]
    init_beta: 0.8


  beta-lstm: #時定数betaを推定するモデル (忘却ゲート推定モデル)
    window: &window 3 #beta推定時に注目するeventのtimestep
    in_size: *in_size
    in_channel: *window 
    cnn_hiddens: [4,8,12]
    pool_type: avg 
    is_bn: True
    dropout_rate: 0.2
    lstm_hidden: 256
    lstm_num: 3
    range_out: 0.2 #出力の最大・最小の絶対値


  gamma-cnn: #膜抵抗γを推定するモデル (入力ゲート推定モデル)
    in_size: *in_size
    in_channel: 2 #positiveとnegativeで2つ
    hiddens: [4,8]
    out_channel: *out_channel
    pool_type: avg 
    is_bn: True
    dropout_rate: 0.2
  

  snn: #分類を行うSNN
    in_size: *gamma_in
    in_channel: *out_channel
    out_size: 11 #分類するクラス数
    hiddens: [16,32,64]
    linear_hidden: 256 #flattenした後の線形層
    pool_type: avg 
    is_bn: True
    beta: 0.5
    mem_threshold: 1.0
    dropout_rate: 0.2


  loss-param:
    alpha1: 0.1
    rho-mean: 0.1
    k-beta: 8 #tanhのスケール
    s-beta: -4 #tanhのシフト

    alpha2: 0 #gammaの目的関数は使わない
    rho-max: 0.05
    gamma-max: 0.25 #これも0.5から1/2にした. これにより, 入力ゲートが抑えられる
    k-gammma: 14 #シグモイドのスケーリング係数
    s-gamma: -7 #シグモイドのシフト
